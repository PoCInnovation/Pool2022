{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4465bcd3",
   "metadata": {},
   "source": [
    "# POC - AI Pool 2022 - Day 01 - Multinomial Naive Bayes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.\n",
    "\n",
    "Naive Bayes classifier is a collection of many algorithms where all the algorithms share one common principle, and that is each feature being classified is not related to any other feature. The presence or absence of a feature does not affect the presence or absence of the other feature.\n",
    "\n",
    "Multinomial Naive Bayes explained [here](https://www.youtube.com/watch?v=O2L2Uv9pdDA)\n",
    "\n",
    "### What are you going to do ?\n",
    "\n",
    "We are going to perform a multinomial Bayesian classification in order to be able to classify a sms as a spam or as a ham (Ham is a message which is not a spam).\n",
    "\n",
    "To do this, you will first process the dataset: ``./data/SMSSpamCollection`` so that it best fits our application. Then we will apply Bayesian classification on test data to evaluate the performance of our program.\n",
    "\n",
    "## I) Data preprocessing\n",
    "\n",
    "### Import\n",
    "\n",
    "For the first cell, we are going to import Pandas, you used it during the data science activity.\\\n",
    "Pandas will allow us to manipulate the data in order to implement our algorithm.\n",
    "\n",
    " * [Pandas documentation](https://pandas.pydata.org/)\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " * Import ``pandas`` and declare an alias as ``pd``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870695fc",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "We are going to load the file ``data/SMSspamCollection`` in a dataframe with ``pandas``.\\\n",
    "if you have observed the file, the columns are separated by a ``\\t`` we will have to indicate it to pandas.\\\n",
    "In general datasets contain a header, that is the first line which instead of having values contains the name of the columns. However the file does not contain a header, so we will have to indicate that our file doesn't contain header and indicate the name of the columns when loading the dataset.\n",
    "\n",
    " * [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    " \n",
    "**Exercice :**\n",
    "\n",
    " * 1. Load the file ``data/SMSspamCollection`` and name the columns ``['Label', 'SMS']``.\\\n",
    " * 2. Check the sep, header and names parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_spam = None # <- Code here\n",
    "\n",
    "assert sms_spam['SMS'][1] == \"Ok lar... Joking wif u oni...\", \"You failed to load the dataset, check the read function from pandas\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ffad1",
   "metadata": {},
   "source": [
    "### Data analyse\n",
    "\n",
    "**Here is a description of our dataset:**\n",
    "\n",
    "|Value|Meaning|\n",
    "|:---:|:-----:|\n",
    "|Label| **``ham``** is a sms which is not a spam, then **``spam``** is a spam|\n",
    "|SMS|Content of the message.|\n",
    "\n",
    "We obtained a dataframe following the ``read_csv`` function. A dataframe is composed of different columns which are ``Series`` and which have different methods, for example you will use the ``value_counts`` method in order to see the percentage of spam and ham sms in the dataset.\n",
    "\n",
    " * docs : [value_counts](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)\n",
    "\n",
    "**Exerice :**\n",
    "\n",
    " * Use the ``values_counts`` method with normalization set to True on the ``Label`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27139205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution = None # <- Code here\n",
    "\n",
    "assert distribution[0] == 0.8659368269921034 and distribution[1] == 0.13406317300789664, \"You should check the method value_counts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782b2cd",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "When working on machine learning algorithms, it is necessary to split the data into two parts, the test data and the training data.\n",
    "\n",
    "We must have more training data than test data, in general we take 80% of the data for training and the rest for testing.\n",
    "\n",
    "The objective with this method is that when testing our algorithm, the algorithm never sees the data we send it. In this way, it is not possible that the algorithm has learned the data by heart and so we confirm that the algorithm works in a general way.\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " * 1. take the ``4458`` first sample of data for training, and the rest to test data, you could use slice operator.\n",
    " * 2. the dimension should be ``(4458, 2)`` for training and ``(1114, 2)`` for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e33313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the dataset\n",
    "data_randomized = sms_spam.sample(frac=1, random_state=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "training_set = None # <- Code here\n",
    "test_set = None # <- Code here\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)\n",
    "\n",
    "assert training_set.shape == (4458, 2) and test_set.shape == (1114, 2), \"You should have theses shapes, look instructions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec1c40",
   "metadata": {},
   "source": [
    "### Verify data\n",
    "\n",
    "When splitting the data for classification, it is important to ensure that the training and test data respect the overall distribution of the data. We have previously observed that 13.5% of messages are spam and 86.5% are not. If your data does not respect this distribution, rework the previous part to have approximately the same distribution as the base dataset.\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " * 1. Re-use the methode used in Data analyse part on ``training_set``, with values_counts.\n",
    " * 2. Re-use the methode used in Data analyse part on ``test_set``, with values_counts.\n",
    " \n",
    "You should reload the dataset and all the previous cell after a fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distrib = None # <- Code here\n",
    "test_distrib = None # <- Code here\n",
    "\n",
    "assert train_distrib[0] > 0.8 and test_distrib[0] > 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f72c59",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "If you look at the dataset using a ``training_set.head()``, you will observe that the sentences have punctuation, which is totally useless to our algorithm. That's why we're going to clean up the dataset by removing the punctuation using ``str.replace('\\W', ' ')`` on the ``SMS`` column and applying ``str.lower()`` in the process, which will turn all the letters into lower case.\n",
    "\n",
    " * [Regular expression -> \\W](https://www.geeksforgeeks.org/javascript-regexp-w-metacharacter/)\n",
    " * [pd.Series.str.replace()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html)\n",
    " * [pd.Series.str.lower()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html)\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " * 1. Replace non-word characters with spaces, by applying ``str.replace('\\W', ' ')`` to ``training_set['SMS']``.\n",
    " * 2. Apply the ``str.lower()`` method to ``training_set['SMS']``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae32423",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS'] = None # <- Code here\n",
    "training_set['SMS'] = None # <- Code here\n",
    "training_set.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfa4a45",
   "metadata": {},
   "source": [
    "### Creating the Vocabulary\n",
    "\n",
    "The first step is to create a vocabulary, the vocabulary is a list of unique values of all words.\n",
    "\n",
    "Indeed, we want to obtain the probability that a word appears in a sms.\\\n",
    "To do this we need to count the number of occurrences of each word in our train dataset.\n",
    "\n",
    "* [delete doublon in python list](https://www.w3schools.com/python/python_howto_remove_duplicates.asp)\n",
    "\n",
    "**Exercice :**\n",
    " * 1. Add each word of the training_set to the ``vocabulary`` list.\n",
    " * 2. Remove duplicates from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c57bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for sms in training_set['SMS']:\n",
    "    for word in sms:\n",
    "        pass # # <- Code here : Add each word in vocabulary list\n",
    "        \n",
    "# <- Code here : Delete doublon in the vocabulary list\n",
    "\n",
    "\n",
    "assert len(vocabulary) == 7783, \"The vocabulary should have a lenght of 7783\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ffb04",
   "metadata": {},
   "source": [
    "### Fill vocabulary\n",
    "\n",
    "Now we will count the number of occurrences of each word in each sentence of the train dataset.\n",
    "\n",
    "The final goal is always to have the probability that a word appears in a sentence.\\\n",
    "Check the code and go next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "\n",
    "for index, sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "assert word_counts_per_sms['and'][-2] == 1, \"You did not reach the expected behavior.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19490485",
   "metadata": {},
   "source": [
    "### Adapt dataset shape\n",
    "\n",
    "We are going to concatenate the training dataframe with the one created in the previous cell in order to gather all our information in a single dataframe, we will thus have access to the number of occurrences of each word for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53957f5a",
   "metadata": {},
   "source": [
    "## II) Multinomial Naives Bayes\n",
    "\n",
    "Now, we will apply Multinomial Naive Bayes algorithm\n",
    "\n",
    "### Calculating Constants First\n",
    "\n",
    "Now that we're done with cleaning the training set, we can begin coding the spam filter.\\\n",
    "The multinomial Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:\n",
    "\n",
    "\n",
    "$\\boldsymbol{\\mathbf{P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)}}$\n",
    "\n",
    "$\\boldsymbol{\\mathbf{P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)}}$\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we'll need to use these equations:\n",
    "\n",
    "$\\boldsymbol{\\mathbf{P(w_i|Spam) = \\frac{N_{w_i|Spam}  + \\alpha}{N_{Spam} +  \\alpha \\cdot N_{Vocabulary}}}}$\n",
    "\n",
    "$\\boldsymbol{\\mathbf{P(w_i|Ham) = \\frac{N_{w_i|Ham}  + \\alpha}{N_{Ham} +  \\alpha \\cdot N_{Vocabulary}}}}$\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. As a start, let's first calculate:\n",
    "\n",
    " * P(Spam) and P(Ham)\n",
    " \n",
    " * NSpam, NHam, NVocabulary\n",
    " \n",
    "It's important to note that:\n",
    "\n",
    " * NSpam is equal to the number of words in all the spam messages — it's not equal to the number of spam messages, and it's not equal to the total number of unique words in spam messages.\n",
    "\n",
    "* NHam is equal to the number of words in all the non-spam messages — it's not equal to the number of non-spam messages, and it's not equal to the total number of unique words in non-spam messages.\n",
    "\n",
    "We'll also use Laplace smoothing and set $\\alpha=1$ for avoiding multiplication per 0.\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " - 1. Calculate P(Spam) : $$\\frac{len(spam\\_messages)} {len(training\\_set\\_clean}$$\n",
    " - 2. Calculate P(Ham) : $$\\frac{len(ham\\_messages)} {len(training\\_set\\_clean}$$\n",
    " - 3. Set ``n_vocabulary`` equal to vocabulary's length.\n",
    " - 1. Set ``alpha`` to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "ham_messages = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = None # <- Code here\n",
    "p_ham = None # <- Code here\n",
    "\n",
    "# N_Spam\n",
    "n_words_per_spam_message = spam_messages['SMS'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_words_per_ham_message = ham_messages['SMS'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "# N_Vocabulary length\n",
    "n_vocabulary = None # <- Code here\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = None # <- Code here\n",
    "\n",
    "assert alpha == 1 and p_spam == 0.13458950201884254 and p_ham == 0.8654104979811574 and n_vocabulary == 7783, \"One of these values is wrong.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba88987",
   "metadata": {},
   "source": [
    "### Calculating Parameters\n",
    "\n",
    "Now that we have the constant terms calculated above, we can move on with calculating the parameters P(wi|Spam) and P(wi|Ham).\n",
    "\n",
    "P($w_i$|Spam) and P($w_i$|Ham) will vary depending on the individual words.For instance, P(\"secret\"|Spam) will have a certain probability value, while P(\"cousin\"|Spam) or P(\"lovely\"|Spam) will most likely have other values.\n",
    "\n",
    "Therefore, each parameter will be a conditional probability value associated with each word in the vocabulary.\n",
    "\n",
    "The parameters are calculated using these two equations:\n",
    "\n",
    "$$\\boldsymbol{\\mathbf{P(w_i|Spam) = \\frac{N_{w_i|Spam} +  \\alpha}{N_{Spam} +  \\alpha \\cdot N_{Vocabulary}}}}$$\n",
    "\n",
    "$$\\boldsymbol{\\mathbf{P(w_i|Ham) = \\frac{N_{w_i|Ham} +  \\alpha}{N_{Ham}  + \\alpha \\cdot N_{Vocabulary}}}}$$\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " - 1. Calculate ``p_word_given_spam``: $$\\frac{n\\_word\\_given\\_spam + \\alpha}{n\\_spam + \\alpha * n\\_vocabulary}$$\n",
    " - 2. Calculate ``p_word_given_ham``: $$\\frac{n\\_word\\_given\\_ham + \\alpha}{n\\_ham + \\alpha * n\\_vocabulary}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a88568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "    p_word_given_spam = None # <- Code here\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "    p_word_given_ham = None # <- Code here\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63fbe7",
   "metadata": {},
   "source": [
    "### Classifying A New Message\n",
    "\n",
    "Now that we have all our parameters calculated, we can start creating the spam filter. \\\n",
    "The spam filter is understood as a function that:\n",
    "\n",
    " * Takes in as input a new message ($w_1$, $w_2$, ..., $w_n$).\n",
    " * Calculates P(Spam|$w_1$, $w_2$, ..., $w_n$) and P(Ham|$w_1$, $w_2$, ..., $w_n$).\n",
    " * Compares the values of P(Spam|$w_1$, $w_2$, ..., $w_n$) and P(Ham|$w_1$, $w_2$, ..., $w_n$), and:\n",
    "   * If P(Ham|$w_1$, $w_2$, ..., $w_n$) > P(Spam|$w_1$, $w_2$, ..., $w_n$), then the message is classified as ham.\n",
    "   * If P(Ham|$w_1$, $w_2$, ..., $w_n$) <= P(Spam|$w_1$, $w_2$, ..., $w_n$), then the message is classified as spam.\n",
    "   \n",
    "**Exercice :**\n",
    "\n",
    " * Complete the condition with : ``if p_ham_given_message is greater than p_spam_given_message``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58983e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message) -> str:\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham: \n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    # complete the instruction\n",
    "    if None: # <- Code here\n",
    "        print('Label: Ham')\n",
    "        return \"ham\"\n",
    "    else:\n",
    "        print('Label: Spam')\n",
    "        return \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c9312",
   "metadata": {},
   "source": [
    "``message`` = ``WINNER!! This is the secret code to unlock the money: C3421.`` and should be classify as ``spam``\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " * Use the classify() function you developped to detect if this ``message`` is a **spam** or **ham**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"WINNER!! This is the secret code to unlock the money: C3421.\"\n",
    "\n",
    "label = None # <- Code here\n",
    "assert label == \"spam\", \"Your algorithm should classify this as spam message.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4dcd6b",
   "metadata": {},
   "source": [
    "message = ``Sounds good, Tom, then see u there`` and should be classify as ``ham``\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " * Use the classify() function you developped to detect if this ``message`` is a **spam** or **ham**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de337b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Sounds good, Tom, then see u there\"\n",
    "\n",
    "label = None # <- Code here\n",
    "assert label == \"ham\", \"Your algorithm should classify this as ham message.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01891e6d",
   "metadata": {},
   "source": [
    "### Measuring the Spam Filter's Accuracy\n",
    "\n",
    "The two results look promising, but let's see how well the filter does on our test set, which has **1,114** messages.\n",
    "\n",
    "Check this function that returns classification labels instead of printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    else:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a42c0",
   "metadata": {},
   "source": [
    "Now that we have a function that returns labels instead of printing them, we can use it to create a new column in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236fe0b",
   "metadata": {},
   "source": [
    "We can compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages.\\\n",
    "To make the measurement, we'll use accuracy as a metric:\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    " - 1. Get the number of correct prediction\n",
    " - 2. Calculate ``p_word_given_ham`` : $\\frac{correct}{total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = None\n",
    "total = test_set.shape[0]\n",
    "correct = 0\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    print(\"Prediction : \" + row['predicted'] + \" -> Label : \" + row['Label'])\n",
    "\n",
    "\n",
    "\n",
    "assert accuracy > 0.98, \"Your accuracy should be greather than 98%.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5ff83",
   "metadata": {},
   "source": [
    "The accuracy is close to 99.00%, which is really good. our filter classified more than 98 percent of the messages correctly.\n",
    "\n",
    "You just created a filter able to classify messages and detect spam using the multinomial naive bayes algorithm.\n",
    "\n",
    "And you also just finished this day, rest well for the next one you, will learn how to create neural networks, exciting no ? :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}