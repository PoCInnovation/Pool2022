{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "In this part we will cover Deep Q-Learning, which is basically Q-learning with neural networks.\n",
    "\n",
    "We will cover:\n",
    "- Bellman Equation\n",
    "- Neural Networks role in Deep Q-Learning\n",
    "\n",
    "And then we will make a whole Deep Q-Learning algorithm using the library [Pytorch](https://pytorch.org/) and the environment framework [OpenAI-Gym](https://gym.openai.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bellman Equation\n",
    "\n",
    "![bellman](./img/bellman_equation.png)\n",
    "\n",
    "The Bellman Equation tells us how to update our Q-table after each step we take. To summarize this equation, the agent updates the current perceived value with the estimated optimal future reward which assumes that the agent takes the best current known action. In an implementation, the agent will search through all the actions for a particular state and choose the state-action pair with the highest corresponding Q-value.\n",
    "\n",
    "This function allows us to determine the expected action based on states.\\\n",
    "In this exercice we will use a slightly modified version of the Bellman Equation adapted for [deterministic environments](https://www.doc.ic.ac.uk/project/examples/2005/163/g0516334/environ.html) such as the one we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks role in Deep Q-Learning\n",
    "\n",
    "While vanilla Q-Learning might be sufficient for learning how to perform in simple environment, its perfromance will drop off considerably in more complex and sophisticated environments.\\\n",
    "The iterative process of updating the Q-Values of each state-action pair on a larger state space will become exponentially inefficient.\n",
    "\n",
    "Rather than using value iteration to compute Q-Values and find the optimal Q-Function, we can instead use a function approximator to estimate the optimal Q-Function.\\\n",
    "And you know what can do an excellent job at approximating functions ? That's right: deep neural networks.\\\n",
    "The neural network will be responsible for **evaluating** states. (ie. determine the expected reward of each action on a givent state)\n",
    "\n",
    "By replacing the Q-Table by an artificial neural network, we get rid (somehow) of the exponentially expensiveness of the original Q-Table, allowing us to make our agent evolve in more complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "Gym is an convenient framework for Reinforcement Learning as it provide a handful of environments for our Agent to evolve in.\n",
    "Gym aims to be a standard in artificial intelligence benchmarking and hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible.\n",
    "The environment we will be using in this exercice is [CartPole-V0](https://gym.openai.com/envs/CartPole-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start!\n",
    "\n",
    "Alright so first things first, let's import everything that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, let's start building our Neural Network.\n",
    "\n",
    "Implement the init fuction of the Deep Q Network.\n",
    "We will have **2 fully connected layers** and **an output layer**.\n",
    "\n",
    "Tips: [torch.nn.Linear](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html), [torch.optim](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # -- Setup the layers (3 lines) --\n",
    "        self.fc1 =\n",
    "        self.fc2 =\n",
    "        self.out =\n",
    "        # -- end of code --\n",
    "\n",
    "        # -- Setup the optimizer and the loss function (2 lines) --\n",
    "        self.optimizer =\n",
    "        self.loss =\n",
    "        # -- end of code --\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next let's implement the forward, you should know how to do it now.\\\n",
    "For the activation function, Relu is suggested for this exercise.\n",
    "\n",
    "> ⚠️ Don't use the activation function on the output layers, we will interpret the result ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, state):\n",
    "# -- implement the forward (~4-8 lines) --\n",
    "   pass\n",
    "# -- end of code --\n",
    "\n",
    "setattr(DeepQNetwork, \"forward\", forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "And that's pretty much it for our network.\n",
    "\n",
    "Now what we need is a class to define our Agent. The agent will be responsible for:\n",
    "* Storing transitions. The Agent will need to memorize transitions as it is its learning material.\n",
    "* Choose an actions. The Agent will take action on an epsilon-greedy based manner, ie. it will sometimes take random actions to get a wider variety of transitions to learn from.\n",
    "* Learn from it's memory (ie. update the network)\n",
    "\n",
    "Let's start the job by initializing our Agent class.\\\n",
    "We start by storing all our hyper parameters, as it's always a good idea to save them in case we'd need them in the future.\\\n",
    "Then we initialize our evaluation network, and the we init the memory with empty arrays.\\\n",
    "It might be advisable to make a Memory class dedicated to storing and batching transitions, however for the sake of simplicity on this exercice, we will use multiple arrays to store transitions. (You are of course free to implement one yourself if you feel like it)\n",
    "\n",
    "Tips: [np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions, max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        self.replace_target = 100\n",
    "\n",
    "        # -- We initialize our Network... (1 line) --\n",
    "        self.Q_eval =\n",
    "        # -- end of code --\n",
    "\n",
    "        # -- ...and the Agent's memory (5 lines) --\n",
    "        self.state_memory =  # /!\\ Reminder: A state is also an array in itself\n",
    "        self.new_state_memory = \n",
    "        self.action_memory = \n",
    "        self.reward_memory = \n",
    "        self.terminal_memory =\n",
    "        # -- end of code --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, let's make a function allowing us to store transitions easily, this one is pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_transition(self, state, action, reward, new_state, terminal):\n",
    "    index = self.mem_cntr % self.mem_size\n",
    "    \n",
    "    # -- store the transition (5 lines) --\n",
    "    self.state_memory[index] = \n",
    "    self.new_state_memory[index] = \n",
    "    self.reward_memory[index] = \n",
    "    self.action_memory[index] = \n",
    "    self.terminal_memory[index] = \n",
    "    # -- end of code --\n",
    "    \n",
    "    self.mem_cntr += 1\n",
    "\n",
    "setattr(Agent, \"store_transition\", store_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Then, we need a function that will actually make a choice based on an observation.\\\n",
    "Remember the epsilon greedy method from the previous subject ? We need to sometime choose a random action according at a rate defined by our epsilon value, this will allow our Agent to explore unknown possibilities to acquire a wider variety of experience.\n",
    "\n",
    "Tips: [torch.Tensor](https://pytorch.org/docs/stable/tensors.html), [torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html), [numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(self, observation) -> int:\n",
    "    # -- Epsilon greedy choice (~7 lines) --\n",
    "    pass\n",
    "    # -- end of code --\n",
    "\n",
    "setattr(Agent, \"choose_action\", choose_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here comes the toughest part of our exercice, we need the Agent to be able to learn.\\\n",
    "In terms of algorithm, it is pretty similar to the typical supervised learning: determine loss -> backward. The catch here is how we determine the loss.\\\n",
    "Basically here we want to move the estimate value of the current state toward the estimate value of the next state, so our target here will be the  (read this again if needed, it is important)\n",
    "\n",
    "Let's break down the algorithm into multiple parts:\n",
    "1. If not enough data in memory, return.\n",
    "2. Retreive a batch of data.\n",
    "3. Feedforward our batch to get the relevant parameters to compute the loss function.\n",
    "4. Compute the target thanks to the Bellman equation: target = reward + gamma * max(next) (Tip: [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html))\n",
    "5. Compute loss and do the backward propagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self):\n",
    "    # 1. If not enough data, return\n",
    "\n",
    "    if self.mem_cntr < self.batch_size:\n",
    "        return\n",
    "\n",
    "    self.Q_eval.optimizer.zero_grad() # (we also reset the optimizer's gradients)\n",
    "\n",
    "    # 2. Retreive a batch of data.\n",
    "\n",
    "    max_mem = min(self.mem_cntr, self.mem_size)\n",
    "    batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "    batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "\n",
    "    # -- We initalize the batch (5 lines) (tip: use the batch variable set above) --\n",
    "    state_batch = \n",
    "    new_state_batch = \n",
    "    reward_batch = \n",
    "    terminal_batch = \n",
    "\n",
    "    action_batch = \n",
    "    # -- end of code --\n",
    "\n",
    "    # 3. Feedforward\n",
    "\n",
    "    # -- We get our relevant parameters for our loss function. (2-3 lines) --\n",
    "    q_eval = \n",
    "    q_next = \n",
    "    # -- end of code --\n",
    "\n",
    "    q_next[terminal_batch] = 0.0 # We set the expected reward of terminal states to 0\n",
    "\n",
    "    # 4. Compute the target\n",
    "\n",
    "    # -- We get our target (what we want to tilt our estimates towards) (1 line) --\n",
    "    q_target =\n",
    "        # Correction note: we transform q_next from a (64, 2) to a (64,) by using T.max, which will select only the highest expected values.\n",
    "    # -- end of code --\n",
    "\n",
    "    # 5. Backward\n",
    "\n",
    "    loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "    loss.backward()\n",
    "    self.Q_eval.optimizer.step()\n",
    "\n",
    "    self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "setattr(Agent, \"learn\", learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now is the moment of truth, run the cells below to start the training of your Agent.\\\n",
    "If everything is going well, your average score should be rapidly approaching 200, which is the highest score you can get on the CartPole environment used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=2, eps_end=0.01, input_dims=4, lr=0.001)\n",
    "scores, eps_history = [], []\n",
    "n_games = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_games):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "    scores.append(score)\n",
    "    eps_history.append(agent.epsilon)\n",
    "    avg_score = np.mean(scores[-30:])\n",
    "    print('episode', i, '| score %.2f' % score, '| average score %.2f' % avg_score, '| epsilon %.2f' % agent.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If you saw an improvement during the training of your agent, then congratulation, you made it!\n",
    "Now, wanna see your Agent in action ? Run this cell below to watch your Agent play 5 games of CartPole.\n",
    "\n",
    "> DISCLAIMER! Gym is very wanky with Jupyter, do not interrupt this cell and wait for it to finish by itself or the render window might never close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_env = gym.make('CartPole-v0')\n",
    "for i in range(5):\n",
    "    score = 0\n",
    "    done = False\n",
    "    observation = new_env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = new_env.step(action)\n",
    "        score += reward\n",
    "        observation = observation_\n",
    "        new_env.render()\n",
    "    print('Game', i, 'Score:', score)\n",
    "new_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Seeing your first Deep-Q-learning algorithm in action is pretty cool isn't it ? \n",
    "\n",
    "Let's resume what we learned in this subject:\n",
    "* Q-Learning is fine, but not enough when increasing complexity.\n",
    "* Artificial neural networks being good at approximating functions, they are the perfect match when it comes to replacing the Q-Table by something more viable.\n",
    "* The Bellman Equation makes the learning possible by allowing us to shift the Q-Value of a state-action pair toward the Q-Value of the next state-action.\n",
    "* Reinforcement Learning is cool.\n",
    "\n",
    "Congrats on concluding this subject! I hope that you enjoyed it and learned a lot of stuff.\\\n",
    "For those tough boys(and girls) that want EVEN MORE Reinforcement Learning action, the last subject of today will give you carte blanche to try and solve an harder environment."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
