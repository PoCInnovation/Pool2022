{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "\n",
    "Hello, today we'll learn about reinforcement learning. Here you will learn how to train an AI to solve its environment.\n",
    "Reinforcement Learning is used in autonomous cars and video games.\n",
    "\n",
    "This workshop is separated in two parts:\n",
    "\n",
    "* Basic theory behing reinforcement learning.\n",
    "  \n",
    "* Making of an AI that can solve a labyrinth.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the duration of this workshop, I'll use some abreviations :\n",
    "\n",
    ">RL\n",
    ": *Reinforcement Learning*\n",
    ">DL\n",
    ": *Deep Learning*\n",
    ">MDP\n",
    ": *Markov Decision Process*\n",
    "\n",
    "\n",
    "Reinforcement Learning is a subset of machine learning. In the domain of automated learning, we distinguish Supervised Learning and Unsupervised Learning. RL is closer to Unsupervised Learning, but not quite the same.\n",
    "Just like Supervised Learning, RL uses data to improve, but a non-labeled one. Thus, the learning process is fundamentally different.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby_img](./img/supervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Basics\n",
    "\n",
    "We will now see the fundamentals behind Reinforcement Learning: the Markov Decision Process (MDP), which is the single most important concept behind (almost) every type of RL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (MDP)\n",
    "\n",
    "\"In mathematics, a **Markov decision process (MDP)** is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.\n",
    "\n",
    "At each time step, the process is in some state **s**, and the decision maker may choose any action **a** that is available in state **s**. The process responds at the next time step by randomly moving into a new state **s'**, and giving the decision maker a corresponding reward **Ra(s,s')**.\n",
    "\n",
    "The probability that the process moves into its new state **s'** is influenced by the chosen action. Specifically, it is given by the state transition function **Pa(s,s')**. Thus, the next state **s'** depends on the current state **s** and the decision maker's action **a**. But given **s** and **a**, it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.\n",
    "\n",
    "Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. \"wait\") and all rewards are the same (e.g. \"zero\"), a Markov decision process reduces to a Markov chain.\"\n",
    "\n",
    "    - Wikipedia\n",
    "\n",
    "Sounds complicated, huh ?\n",
    "\n",
    "Let's separate this topic into multiple part, shall we ?\n",
    "\n",
    "1. Markov Process\n",
    "2. Markov Reward Process\n",
    "3. Markov Decision Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Markov Process\n",
    "\n",
    "\"A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\"\n",
    "\n",
    "    - Wikipedia\n",
    "  \n",
    "In other words, the Markov Process is a system where, when in the state S, the probability to move to a new state S' is directly correlated to S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's take an example. Imagine a robot moving through a 9-square labyrinth. You are an observer watching the movement of the robot each moment **t**. The robot and the labyrinth represents the system, and the state of the system is represented by the position of the robot in the labyrinth at a moment **t**. Each moment, the robot move from square to square (thus from state to state) following rules.\n",
    "\n",
    "All possible state of a system is called the **State Space**. In this example, the **State Space** is of size 9, because there is only 9 square in the labyrinth. Below, we represented the state change of the robot over time. A sequence of observation over time is called **Chain of State**, and form what we call an **history**.\n",
    "\n",
    "---\n",
    "![maze_img](./img/maze.gif)![transition_img](./img/transition.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We usually represent a system and it's state change by a graph, in which each node is a state.\n",
    "\n",
    "The probability to go from a state to another is defined by the system (ie. the robot and the labyrinth). And the observer (you) don't have access to those probabilities. He can only observe and estimate said probabilities.\n",
    "\n",
    "The more observation we get, the more our approximation will be close to the real probabilities. In our example, we will define the probabilities of the system. Thus we will have access to the real probabilities, but it is really important to distinguish them from the estimated probabilities. The real probabilities are fixed and doesn't change, while the estimated probabilities evolve as we get more and more observations. Let's nor forget that the system is unchanging, we only observe it.\n",
    "\n",
    "Another important element is teh **Markov Property**. For a system to be called a Markov Process, the states must be unique and distinguishable from each others. This allows us having to know only one and unique state to predict the next state of the system. Thus not having to know the history to do a prediction on the system.\n",
    "\n",
    "> A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present values) depends only upon the present state; that is, given the present, the future does not depend on the past. A process with this property is said to be Markovian or a Markov process. The most famous Markov process is a Markov chain.\n",
    "[*Markov Property Wikipedia*](https://en.wikipedia.org/wiki/Markov_property)\n",
    "\n",
    "![proba](./img/proba.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markov Reward Process\n",
    "\n",
    "> A **transition** signifies moving from a state to another.\n",
    "> An **episode* is a sequence of transition. (example: wake up => code => deploy => sleep)\n",
    "\n",
    "\n",
    "Now that we have the basics, let's introduce the next element: **the rewards**. We will associate a reward with each **transition**. Reward are a number, that can be positive (good reward) or negative (bad reward).\n",
    "\n",
    "Our objective will be to maximise the amount of reward acquired during the **episodes**.\n",
    "\n",
    "For each episode, we compte the efficiency *G*, which is the sum of the rewards. The goal is to **maximise** this efficiency.\n",
    "\n",
    "![return](./img/return.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another rather important element is what we call the **discount factor**, usually noted Î³ (gamma). It's a number between 0 and 1.\n",
    "\n",
    "Let's try to understand the calculation below.\n",
    "For each transition of our **State Space**, we store the reward of the state *t + 1*. Let's take another example with a new system closer to real life.\n",
    "\n",
    "![reward](./img/reward.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each element of our episode, we compute the **efficiency** as the sum of futur rewards.\n",
    "\n",
    "Let's look at an example with this episode (wake up => code => deploy => sleep) : given that our episode starts with the wake up state, what is the estimated efficiency in this state ?\n",
    "\n",
    "In other words, we will compute the sum of the reward of all states of this episode: -3 + 10 + 10 = 10.\n",
    "\n",
    "But something is missing, rewards coming in the near future are more important than the ones coming in a distant future.\n",
    "Reward coming in the futur will thus be multiplied by the **discount factor** (gamma) to the power to *t*, *t* being the time before the reward is obtained. We obtain different results depending on the value of gamma:\n",
    "\n",
    "* If gamma is equal to 1, the efficiency will be equal to the sum of all the future reward of the episode.\n",
    "\n",
    "* If gamma is equal to 0, the efficiency will be equal to the immediate reward.\n",
    "\n",
    "The **discount factor** is a very important parameter in RL. Think of it as the distance your agent see in the future to estimate efficiency.\n",
    "\n",
    "![return](./img/return.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Still awake ? You're almost there !\n",
    "\n",
    "* For now the efficiency G isn't very useful, because it is defined only for specific episodes we observe.\n",
    "Thus, the efficiency G is calculated for each specific episode, and can vary from an episode to another, even for an identical state.\n",
    "\n",
    "* However, if we compute the **efficiency expecation** for each state of the system, that is to say by doing the mean of a large amount of episode, we get a very useful value called the **state value** which tells us how much it is good to be in a particular state.\n",
    "\n",
    "For each state of the system, the value *v(s)* is the mean of the efficiencies G of this state *s* from all the episode we gather.\n",
    "\n",
    "![value](./img/valuefunction.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.  Markov Decision Process\n",
    "\n",
    "> Here, a **transition** corresponds to this: (state => action => new state => reward)\n",
    "\n",
    "Here we will talk on how to extends our Markov Process to a Markov **Decision** Process.\n",
    "\n",
    "* First, we will add a set of action which must have a fixed size. That's what we call the **Action Space**. (For example, in the snake game, the action space is [Moving Up, Moving Down, Moving Left, Moving Right], and have a size of 4).\n",
    "\n",
    "* Second, we will link actions with state changes and rewards.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![markov](./img/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "In the case of a Markov Process we use a transition matrix, a 2-dimensional array which contains the **real probabilities** to go from a state to another in the system. For example with the diagram below, the probability to go from state *s1* to state *s2* is *p12*.\n",
    "\n",
    "![process](./img/matrice.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we added actions, we need to modify our matrix to take them in account. Therefore we are going to go from a 2-dimensionnal matrix to a 3-dimensionnal one.\n",
    "\n",
    "We no longer passively observe the system: we can actively choose to do an action at each state change.\n",
    "\n",
    "We have a matrix which a 1st dimension represents the actions the agent can take and the 2nd and 3rd represents the original state (before taking the action) and the state of arrival (after taking the action). Below a summary diagram.\n",
    "\n",
    "![mdp_pro](./img/mdp_rpo.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Congratulations!**. We covered to most important part of RL and MDP. The last thing we have to talk about is **Policy** and you will have the complet vocabulary of Reinforcement Learning (well not really but the basics at least)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get to work !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to make use of the **Value Function** to solve a maze. An **Agent** will walk around a maze in which there is a **Target** he will have to go look for. The **Target** give a reward of 10 and the fire a reward of -5. The **Agent** will walk around thousands of time in the maze and learn step by step to solve the state. You will learn that the **Value Function** is **the brain** of your **Agent**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing some Tools that will be useful along with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "from Envi import Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Disclaimer: Be mindful of the return type of each function. A list containing a single int is different from an int !**\n",
    "\n",
    "the first take is to define some hyperparameters as well as a **Named Tuple** named `Transition` representing the transitions (`state`, `action`, `next_state`, `reward`). We will use this named tupe like a 4-dimensional list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"state\", \"action\", \"next_state\", \"reward\")\n",
    "BATCH_SIZE = # Define batch size\n",
    "GAMMA = # Define gamma\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental concept of RL is that the Agent must own a **memory**. This memory will store transitions the Agent will go through. This will allow us to update the **Value Function** of this transition. This allow the Agent to train multiple time on the same transition that it would see only a feww times.\n",
    "\n",
    "We will therefore create a `Memory` class.\n",
    "\n",
    "Initialize this class with:\n",
    "\n",
    "* A variable `capacity`, containing le number of transitions the memory can store. When it is full, the memory will erase the oldest transition stored.\n",
    "\n",
    "* A variable `memory` containing the transitions list (the memory itself).\n",
    "\n",
    "* A variable `position` that will store the position of the last added transition in memoru. This will allow us to add transistion at right place and to remove the oldest ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        # Need some code\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory: list = []\n",
    "        self.position: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_memory : Memory = Memory(1000)\n",
    "\n",
    "assert (test_memory.capacity == 1000)\n",
    "assert (test_memory.memory == [])\n",
    "assert (test_memory.position == 0)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We will create a `push()` and `Memory`. This function will take as parameters the necessary data to create our transition (param1=state, param2=new_state, ...). We start by checking if the size of the memory is lower than its capacity. If that's the case, we add a `None` to the memory to create a new empty space.\n",
    "\n",
    "* Then, we will add to the memory the new transition to the memory space pointing by `position`.\n",
    "\n",
    "* Finally, we can update `position` following the rule: The position increases by 1, but if it is greater than the capacity, the position goes back to 0 (use modulo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(self, *args: list):\n",
    "    \"\"\"push a transition\"\"\"\n",
    "    # Need some code\n",
    "\n",
    "setattr(Memory, 'push', push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy_state = [43]\n",
    "dumy_action = [3]\n",
    "dumy_next_state = [44]\n",
    "dumy_reward = [10]\n",
    "\n",
    "dumy1_state = [28]\n",
    "dumy1_action = [3]\n",
    "dumy1_next_state = [44]\n",
    "dumy1_reward = [-5]\n",
    "\n",
    "\n",
    "test_memory : Memory = Memory(2)\n",
    "test_memory.clear()\n",
    "test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward)\n",
    "\n",
    "assert (test_memory.position == 1)\n",
    "assert (test_memory.memory == [Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "\n",
    "test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward)\n",
    "assert (test_memory.position == 0)\n",
    "assert (test_memory.memory == [Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward), Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "\n",
    "test_memory.push(dumy1_state, dumy1_action, dumy1_next_state, dumy1_reward)\n",
    "assert (test_memory.position == 1)\n",
    "assert (test_memory.memory == [Transition(dumy1_state, dumy1_action, dumy1_next_state, dumy1_reward), Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then implement two new useful functions. The `batch()` function which will return a **Sample** of *n* transitions, taken randomly in the Agent's memory. This will allow us to update the value of the states with new data. You can use `random.sample()` to help you implement the batch function. Also implement the `__len__()` operator overload, which returns the current size of the memory.\n",
    "Click [here](https://www.geeksforgeeks.org/operator-overloading-in-python/) for more information about operator overloading in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(self, batch_size):\n",
    "    return # Need some code\n",
    "\n",
    "def __len__(self):\n",
    "    return # Need some code\n",
    "\n",
    "setattr(Memory, '__len__', __len__)\n",
    "setattr(Memory, 'batch', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy_state = [43]\n",
    "dumy_action = [3]\n",
    "dumy_next_state = [44]\n",
    "dumy_reward = [10]\n",
    "\n",
    "test_memory.capacity = 20\n",
    "test_memory.clear()\n",
    "[test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward) for i in range(20)]\n",
    "\n",
    "dumy_batch = test_memory.batch(10)\n",
    "\n",
    "assert (len(dumy_batch) == 10)\n",
    "for i in range(10) :\n",
    "     assert(dumy_batch[i].state == [43] and dumy_batch[i].action == [3] and dumy_batch[i].next_state == [44] and dumy_batch[i].reward == [10])\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start working on the main class: the `Agent` class.\n",
    "\n",
    "* This class take the **environment** as a parameter in its constructor, which is an `Env` instance.\n",
    "\n",
    "* Create a variable `memory` which contains an instance of the `Memory` class. This memory can have a capacity of 200 for the moment.\n",
    "\n",
    "* Create a variable `env` to store the received environment.\n",
    "\n",
    "* Create a variable `V` which store as list full of zeros, and of which the size is the same size of the **State Space** of the environment.\n",
    "\n",
    "* Finally, call the function `self.init_v()` which will initialize the **Q-Table** (ie. values of the states) where **Targets** and **fires** are with their default value. These values are equal to the reward these states give."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: Env):\n",
    "        # Need some code\n",
    "        self.init_v()\n",
    "\n",
    "    def init_v(self):\n",
    "        map = env.get_map()\n",
    "        for j in range(len(map)):\n",
    "            if map[j] == 'T':\n",
    "                self.V[j] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "test_agent : Agent = Agent(env)\n",
    "\n",
    "assert (test_agent.memory.capacity == 2000)\n",
    "assert (len(test_agent.V) == len(env.get_map()))\n",
    "assert (test_agent.env != None)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already have access to some utility functions to **Save** and **Load** the values of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_v(self, path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        for v_f in self.V:\n",
    "            f.write(str(round(v_f, 4)) + \" \")\n",
    "\n",
    "def load_v(self, path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        self.V = f.read().split(\" \")\n",
    "        del self.V[len(self.V) - 1]\n",
    "        for k in range(len(self.V)):\n",
    "            self.V[k] = float(self.V[k])\n",
    "\n",
    "setattr(Agent, 'save_v', save_v) \n",
    "setattr(Agent, 'load_v', load_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RL, it is important that the agent doesn't always blindly take actions with the highest expected reward. You have to add reasonable amount of randomness to allow your agent to explore every possibilities.\n",
    "The chances that the Agent takes a random action instead of the best known one is what we call **epsilon**. At the beggining, *epsilon* is very close to 1, meaning that he will only take random actions, allowing him to quickly explore a large amount of different states. As time goes on, we gradually decrease *epsilon* to allow our Agent to use all the knowledge he acquired, that is to say, to use the value function.\n",
    "\n",
    "* Let's start by implementing an array of actions. There is 4 possible actions: Up, Down, Left, Right (1, 2, 3, 4). We store this array in a variable called `actions`.\n",
    "\n",
    "* Now comes the hard part, we want to know what is the best action the Agent must take according to its past experience. We know that the value allows us to know how much it is good to be in a state. The environment contains a `predict(action)` function which returns a list with a single element: the state the agent will end up in if it takes the action.\n",
    "\n",
    "* We also know thanks to our **Q-Table** that each state has a **value**. Therefore, we will iterate through every actions and check in which state the Agent end up, then we will look at the value of each of these states thanks to **self.V[s]**. Finally, we will return the action that takes the agent to the state with the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_step(self) -> int:\n",
    "    actions = [1, 2, 3, 4]\n",
    "    # Need some Code\n",
    "    # Tips: Predict takes an action and return [next_state of agent] \n",
    "    # Tips : self.V is an array with value for all state so self.V[state] return the value function for that state\n",
    "    return # Return Action (an int between 1 and 4)\n",
    "\n",
    "setattr(Agent, 'greedy_step', greedy_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "\n",
    "test_agent.load_v(\"test.txt\")\n",
    "assert(test_agent.greedy_step() == 2)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now implement the `take_action()` function which will perform **actions**. It receives the state of the agent as long with *epsilon*.\n",
    "\n",
    "* We will take random number between 0 and 1, if this number is lower than **epsilon** the agent takes a random action.\n",
    "\n",
    "* If the number is greater than **epsilon**, we will call our `greedy_step()` to take the best known action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCLAIMER: Pay attention to the return type of the function.\n",
    "\n",
    "def take_action(self, eps_t: int) -> [int]:\n",
    "    # Need some code (if .... return/ else .... return WARNING: cf return type)\n",
    "    # Tips second line : Need to use greedy_step function that return an int between 1 and 4 and wrap that int into an array ([int])\n",
    "    # Tips first line : random.randint/random.uniform \n",
    "\n",
    "setattr(Agent, 'take_action', take_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "\n",
    "test_agent.load_v(\"test.txt\")\n",
    "assert(test_agent.take_action(0) == [2])\n",
    "assert(test_agent.take_action(1)[0] >= 1 and test_agent.take_action(1)[0] <= 4)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quick reminder:\n",
    "In Order to acquire reward, the Agent must maximize it's efficiency. The value function is the way to define the value of a state, in other word, how much this state contribute to maximize efficiency. We denote this function *V(s)*. It measures future reward the Agent could possibly obtain being in the state *s*.\n",
    "![rappel fonction de valeur](./img/rapp.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is a reminder on how to calculate the efficiencer of a state:\n",
    "![calcul rendement](./img/return.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Also remember that the value of a state is equal to the expectation of all the efficiencies of this states on every episodes.\n",
    "![calcul value](./img/valuefunction.png)\n",
    "\n",
    "\n",
    "\n",
    "> We can now tweak this expression. We update the value of each state thanks to this formula that take the two previous ones. We take the value of the actual state and add its reward, as well as *gamma* multiplied by the difference between the value of the state *t + 1* and the actual state.\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V(s) = V(s) + r + \\gamma * [V(s') - V(s)]\">\n",
    "\n",
    "> We repeat this operation as many times as we have transitions, in order to adjust teh values of each visited states, and so approach teh real expectation of this state.\n",
    "\n",
    "\n",
    "> However, juste like with the parameters of a neural network, we have to use a **Learning rate** which will allow to have more stable adjustments.\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V(s) = V(s) + \\alpha * [r + \\gamma * [V(s') - V(s)]]\">\n",
    "\n",
    "\n",
    "> *r* = reward\n",
    "> *s* = state\n",
    "> *s'* = next_state\n",
    "> *gamma* = gamma\n",
    "> *alpha* = Learning rate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, implement the `learn()` function of the agent. This function is responsible for updating the **Values** of each state.\n",
    "\n",
    "* Retreive BATCH_SIZE transitions from the agent Memory, using the function `batch()` that you implemented earlier.\n",
    "\n",
    "* You'll have to use some **prints** to test out what you retreived from the batch function. For now, you retreived a list of named tuples `Transition`, but we want a single tuple `Transition` containing lists. To acheive this, we are going to use the `*` operator and the `zip()` function.\n",
    "\n",
    "* You now have to iterate through `batch.state`, `batch.next_state` and `batch.reward`. You can use a for loop with `zip` and retreive an element from the lists for each loop.\n",
    "\n",
    "* Initialize three variable `s`, `s_prime` and `reward` with the first element of the respective lists (`state`, `next_state` and `reward`).\n",
    "\n",
    "* Finally, calculate the new value of `self.V[s]` with the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self):\n",
    "    transitions = self.memory.batch(BATCH_SIZE)\n",
    "    #We go from an array of transition to a transition of array.\n",
    "    #Lookup python unpacking and the zip() function for more information.\n",
    "    batch: Transition = Transition(*zip(*transitions))\n",
    "    for state, next_state, reward in zip(batch.state, batch.next_state, batch.reward):\n",
    "        # Need one line of code (look above there is a formula with v[s] = ....)\n",
    "\n",
    "setattr(Agent, 'learn', learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isTired = True\n",
    "assert (isTired == False)\n",
    "print(\"\\n\\033[92mVery good ! go to last step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the last function: the `main` function. In RL in general, the main loop always looks something similar to this.\n",
    "\n",
    "* We start by creating an instance of the environment our Agent will evolve in.\n",
    "\n",
    "* We create an instance of the Agent and pass the environment to it.\n",
    "\n",
    "* We define two variables, one to count the number of action done, and one containing **epsilon**.\n",
    "\n",
    "* Then we start to perform actions.\n",
    "\n",
    "* Every 10 iterations we decrease **epsilon**.\n",
    "\n",
    "* We retreive the **actual state of the agent**. (using the `get_env()` function from the environment).\n",
    "\n",
    "* We ask the Agent which action it will take.\n",
    "\n",
    "* We pass this action to the environment thanks to the `step()` function so the environment will perform the action and return the new state and the obtained reward.\n",
    "\n",
    "* We add the transition our agent just did to it's memory.\n",
    "\n",
    "* We call the `render()` function to print the actual state of the environment.\n",
    "\n",
    "* We don't start learning until we have at least 128 transitions in memory.\n",
    "\n",
    "* Once the Agent pocess enough transition in memory, we will start to make it learn and affine it's **value functions** using these transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env: Env = Env(\"maze.txt\")\n",
    "    agent: Agent = Agent(env)\n",
    "    i = 0\n",
    "    eps = 0.9\n",
    "    #agent.load_v(\"save.txt\")\n",
    "    while i < 10000:\n",
    "        if i % 10 == 0:\n",
    "            eps = max(eps * 0.9998, 0.05)\n",
    "        state: list = env.get_env()\n",
    "        action: list = agent.take_action(eps)\n",
    "        next_state, reward = env.step(action[0])\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        env.render(agent.V, eps, 0.1)\n",
    "        i += 1\n",
    "        if i > 128:\n",
    "            agent.learn()\n",
    "    #agent.save_v(\"save.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
